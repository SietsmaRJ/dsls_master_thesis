{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing some shizzle.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Defining some import and export locations\n",
    "location = 'rjsietsma'\n",
    "read_loc = '/home/'+location+'/Documents/School/Master_DSLS/Final_Thesis/Initial_Data_exploration/'\n",
    "data_expor_loc = '/home/'+location+'/Documents/School/Master_DSLS/Final_Thesis/Past_initial_data/'\n",
    "img_output_dir = '/home/'+location+'/PycharmProjects/dsls_master_thesis/side_scripts/output_img/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function bokeh.io.output.output_notebook(resources=None, verbose=False, hide_banner=False, load_timeout=5000, notebook_type='jupyter')>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some bokeh stuff\n",
    "\n",
    "from bokeh.plotting import figure, ColumnDataSource, output_notebook, show, save\n",
    "from bokeh.models import HoverTool, WheelZoomTool, PanTool, BoxZoomTool, ResetTool, TapTool, SaveTool\n",
    "from bokeh.palettes import inferno\n",
    "\n",
    "output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to perform Shapiro-Wilk, Kolmogorov-Smirnov, Wilcoxon-/Mann-Whitney U -test, and a pearson correlation test.\n",
    "\n",
    "def perform_stats(x, y):\n",
    "    shapiro_x = stats.shapiro(x)[1]\n",
    "    shapiro_y = stats.shapiro(y)[1]\n",
    "    if shapiro_x >= 0.05:\n",
    "        print(\"X is likely normally distributed.\")\n",
    "    else:\n",
    "        print(\"X is likely not normally distributed.\")\n",
    "    if shapiro_y >= 0.05:\n",
    "        print(\"Y is likely normally distributed.\")\n",
    "    else:\n",
    "        print(\"Y is likely not normally distributed.\")\n",
    "    try:\n",
    "        p_ks_xy = stats.ks_2samp(x, y)[1]\n",
    "        print(f\"The Kolmogorov-Smirnov test p-value: {p_ks_xy}\")\n",
    "    except Exception:\n",
    "        print(\"Kolmogorov-Smirnov could not be performed!\")\n",
    "    try:\n",
    "        p_wc_xy = stats.wilcoxon(x, y)[1]\n",
    "        print(f\"The Wilcoxon test p-value: {p_wc_xy}\")\n",
    "    except ValueError:\n",
    "        p_mw_xy = stats.mannwhitneyu(x, y)[1]\n",
    "        print(f\"Wilcoxon could not be performed, \\n\"\n",
    "             f\"Using Mann-Whitney rank test p-value: {p_mw_xy}\")\n",
    "    except Exception:\n",
    "        print(\"Neither Wilcoxon nor Mann-Whitney tests could be performed!\")\n",
    "    try:\n",
    "        p_pears_xy = stats.pearsonr(x, y)\n",
    "        print(f\"The Pearson correlation: {p_pears_xy[0]},\\n\"\n",
    "             f\"p-value: {p_pears_xy[1]}\")\n",
    "    except Exception:\n",
    "        print(\"Pearson correlation could not be performed!\")\n",
    "\n",
    "# Define function to calculate the Z-scores of given data.\n",
    "\n",
    "def calc_z_scores(data):\n",
    "    centered = data - data.mean(axis=0)\n",
    "    return centered / centered.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprint 28-04-2020 - 19-05-2020 (?)\n",
    "=====================\n",
    "\n",
    "- What I want to do:\n",
    "  1. [Focus on fixing missing CAPICE precomputed scores.](#Fixing-CAPICE-precomputed-scores) __Done__\n",
    "      - _Note: Find out how the script of Shuang works on a smaller scale, get an example where a CAPICE score is missing._\n",
    "  2. [Normalize BM_ratio](#Normalize-BM_ratio) for gene length, so $\\frac{n_{malignant}}{n_{benign}}$ per 1000 bases in a gene, instead of [$\\frac{n_{malign}}{n_{benign}} \\times n_{total}$](output_img/bm_ratio.png \"BM_ratio\").\n",
    "  3. _(questionable)_ Fix PCA by identifying categorical features _(Questionable because should I still put time and focus in on the input data?)_\n",
    "      - _Ask Shuang, Joeri and Krista in the PRU 28-04-2020 @ 13:00-14:00_\n",
    "  4. [See what is required](#Required-for-data-sources) for the [data sources](https://docs.google.com/document/d/1D5SiNbeDEfY2hTWquS88MGUrLv6IgZzfCzflppRGb5k/edit) to be implemented into the input data.\n",
    "      - _Note: as this is research, I can use sources that only have precomputed scores available, but just to keep in mind that I should mention that in the discussion._\n",
    "\n",
    "- Notes:\n",
    "  - If I find myself unfocussed:\n",
    "      - Win a game of [solitaire](https://play.google.com/store/apps/details?id=com.lemongame.klondike.solitaire&hl=nl) (game is rigged, not every match is win-able).\n",
    "      - Clean out my closet (it's messy, maybe to clean up the mess inside my head too).\n",
    "      - Go for a walk around the neighbourhood.\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "### Fixing CAPICE precomputed scores\n",
    "\n",
    "Problem: CAPICE precomputed file of all single nucleotide level variants contains caps.\n",
    "\n",
    "Solution: Fix the underlying problem that causes these gaps.\n",
    "\n",
    "Required: \n",
    "- Script to make and batch the CAPICE computing __(positive check)__\n",
    "- Access to GCC cluster __(positive check)__\n",
    "- Knowledge of job scheduler on GCC cluster __(positive check)__\n",
    "- Smaller batches of SNV files to run locally  __(positive check)__\n",
    "\n",
    "Possible cause:\n",
    "- Incorrect batching on tabix.\n",
    "\n",
    "31-04-2020 Problem:\n",
    "- Got it working on Gearshift, but with a problem:\n",
    "    - AttributeError: Can't get attribute 'DeprecationDict' on <module 'sklearn.utils.deprecation' from '/apps/software/PythonPlus/3.7.4-foss-2018b-v20.02.1/lib/python3.7/site-packages/sklearn/utils/deprecation.py'>\n",
    "- Which let's me to beleive that the model was trained in a previous version of sklearn.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "- Update 10:51 : Model compatibility issue. Might have to retrain CAPICE on Gearshift using the latest xgboost, sklearn, scikit-learn, numpy and pandas just to be safe.\n",
    "\n",
    "- Update 12:01 : Managed to get it working using python3.6 on calculon, essentially downgrading my own script (I only used python3.7 for F-strings)\n",
    "\n",
    "- Update 15:55 : Found a very weird bug in the chunking. Seems like the script doesn't like to start at 0 with skiprows and 1000 nrows, solution: nrows - 2 when first iteration, then + 1, then start += nrows + 1...\n",
    "\n",
    "- Update 04-05-2020 : Found out that the first iteration, an overhead of 2 variants was present. Adjusted the start with 2 to compensate. Seemed fine in the first 400000 CADD entries. (no duplicates nor missed entries). __Started full processing__\n",
    "\n",
    "- Update 04-05-2020 : \n",
    "    - Add functionality to continue where left off if it crashes.\n",
    "    - Make dummy script to check for duplicates or if the files are fully complete.\n",
    "    \n",
    "- Update 06-05-2020 : \n",
    "    - Functionaility to continue where left off is required. After 40 hours, it's still processing chromosome 1 after 7 million processed variants. \n",
    "        - Ideas: write json file with start and stop each iteration once that part is processed and added to the larger files. Should overwrite.\n",
    "        - As for now, since this idea is not implemented yet, count the number of rows in the already processed files and set that as start.\n",
    "\n",
    "- Update 11-05-2020:\n",
    "    - The 4 days have ended, chromosome 1 has not been finished yet.\n",
    "    - Output file of chromosome 1 has been renamed to a .tsv file and gzipped (saving about 5 gb of storage).\n",
    "    - Adjustments to the original script have to be made.\n",
    "\n",
    "#### Version 2.0\n",
    "\n",
    "- Added gzip output by default\n",
    "- Currently no function yet to continue where left off.\n",
    "    - Ideas:\n",
    "        - for line in gzip.open(file.gz): nlines+=1 (to trace back where was left off)\n",
    "            - Do not know if this lead to any complications of duplicated entries. Saving the previous results in the RAM memory might solve this to compare to.\n",
    "    - Then in the future:\n",
    "        - Save a json of start and batch size each iteration to mark where the file was left of.\n",
    "- Needs to be added:\n",
    "    - A save of the last precomputed dataframe and a check if any duplicates are in there. Drop if duplicates exist (and warn in logfile). If no previous precomputed dataframe is present, load in the last _batch_size_ of the output files. List the output files by using glob.glob(x, recursive = True). The order has to be specified aswell (np.array(np.arange(1,23).astype('str').tolist() + ['x', 'y']). __This is potentially very dangerous, this is fully under the assumption that the CADD file is ordered on chromosome.__ _Might be fine for now, since the failsave features will be added now and it's only the run with not enough time that failed._ So in short terms:\n",
    "        - \n",
    "    \n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "### Normalize BM_ratio\n",
    "\n",
    "Problem: Larger genes have more benign and malignant samples, because they are larger and thus the chanses of variation on the reference read is higher.\n",
    "\n",
    "Solution: Make a $n_{malignant}$ and $n_{benign}$ per $x$ bases in a gene, where {$x \\in \\mathbb{N}\\ | x \\geq 1 | x \\leq 10000$}\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "### Required for data sources\n",
    "\n",
    "Problem: Now that a lot of data sources have been found, annotation and merging has to be performed in order to use it as training data.\n",
    "- _Additional problem: once a source has been proven beneficial, it has te be implemented to be annotated before capice fits and predicts a new sample._\n",
    "\n",
    "Solution: Find out, feature by feature if genes that have a low AUC improve *__and that genes that have a high AUC do not perform worse__*\n",
    "\n",
    "Required:\n",
    "- Storage \n",
    "    - On GCC cluster for precomputed score files.\n",
    "    - Locally IF file isn't that big or an API can be used.\n",
    "- Data sources __([positive check](https://docs.google.com/document/d/1D5SiNbeDEfY2hTWquS88MGUrLv6IgZzfCzflppRGb5k/edit))__\n",
    "- Script to annotate / merge\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTEPG (Catching The Elusive Predictable Genes) 28-04-2020 - 04-05-2020\n",
    "<br/><br/>\n",
    "\n",
    "What have I done:\n",
    "\n",
    "- [Enrichr Genes < 0.6](https://amp.pharm.mssm.edu/Enrichr/enrich?dataset=b7a959357214069d1905719220ed3e3b)\n",
    "- [Enrichr Genes < 0.7](https://amp.pharm.mssm.edu/Enrichr/enrich?dataset=7d252a3e1df275859228fd562e964344)\n",
    "    - For reference: [Enrichr Genes < 0.85](https://amp.pharm.mssm.edu/Enrichr/enrich?dataset=60c74741c967c25331419bd2b5e7183e)\n",
    "- [Finished investigating initial data sources](https://docs.google.com/document/d/1D5SiNbeDEfY2hTWquS88MGUrLv6IgZzfCzflppRGb5k/edit)\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "What I'm planning to do this week:\n",
    "\n",
    "- [Fixing missing values in precomputed CAPICE scoring file](#Fixing-CAPICE-precomputed-scores). __Done and not done, script has to have some additonal features__\n",
    "    - If stuck, do not keep walking into the wall, continue below and contact Shuang and Joeri\n",
    "- [Run enrichr on increasing cutoffs in AUC (.5, .6, .7 etc.)](#Enrichr-automated-API). __Done__\n",
    "    - Read into the [Enrichr API documentation](https://amp.pharm.mssm.edu/Enrichr/help#api)\n",
    "- Mail Tsjerk that a meeting is apreciated. __Done, no response yet__\n",
    "- Make [suggested plot](https://docs.google.com/presentation/d/1_woKoD9nqWnxueyFYfwyX6B4owg8vYF7rw_nKpMPux4/edit#slide=id.p). __Done__\n",
    "    - Select from enrichr all the genes involved in (lipid) metabolism and see if this explains all / most of the bad performing genes.\n",
    "- Make model on bad performing genes and investigate further.\n",
    "- Start investigating what is required for the data sources to be used in the training phase.\n",
    "- Look into the genes that do perform well without a CGD category, also look into the 1/2 genes that do not perform well __with__ a CGD category\n",
    "- _Optionally_ :\n",
    "    - Create outline for article.\n",
    "    - Adjust BM_ratio plot to normalize for gene length.\n",
    "    - Get the UMCG gene panel lists into python\n",
    "        - Then: investigate which gene panel is performing the worst.\n",
    "        - It would be nice to have this on the side for diagnostics, do __not__ focus on this\n",
    "    - Fix PCA.\n",
    "    - Find possible way to annotate genes for co-factors, sub-family etc.\n",
    "<br/><br/>\n",
    "\n",
    "# CTEPG (Catching The Elusive Predictable Genes) 04-05-2020 - 11-05-2020\n",
    "<br/><br/>\n",
    "\n",
    "What have I done:\n",
    "- [Fixing missing values in precomputed CAPICE scoring file](#Fixing-CAPICE-precomputed-scores).\n",
    "- [Run enrichr on increasing cutoffs in AUC (.5, .6, .7 etc.)](#Enrichr-automated-API).\n",
    "- Make [suggested plot](https://docs.google.com/presentation/d/1_woKoD9nqWnxueyFYfwyX6B4owg8vYF7rw_nKpMPux4/edit#slide=id.p).\n",
    "<br/><br/>\n",
    "\n",
    "Planned for this week:\n",
    "- Release precomputed CAPICE script v2.0\n",
    "- Make model on bad performing genes and investigate further.\n",
    "- Start investigating what is required for the data sources to be used in the training phase.\n",
    "- Look into the genes that do perform well without a CGD category, also look into the 1/2 genes that do not perform well __with__ a CGD category\n",
    "- _Optionally_ :\n",
    "    - Create outline for article.\n",
    "    - Adjust BM_ratio plot to normalize for gene length.\n",
    "    - Get the UMCG gene panel lists into python\n",
    "        - Then: investigate which gene panel is performing the worst.\n",
    "        - It would be nice to have this on the side for diagnostics, do __not__ focus on this\n",
    "    - Fix PCA.\n",
    "    - Find possible way to annotate genes for co-factors, sub-family etc.\n",
    " \n",
    "\n",
    "- Ion transport / transmembrane related genes seems to perform bad. \n",
    "- Make plots combined of all sources, but color them by source.\n",
    "\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Tickets:\n",
    "<br/><br/>\n",
    "\n",
    "## As student / Robert I want to map requirements for SNV and gene features.\n",
    "\n",
    "- Content:\n",
    "\n",
    "Overview of features has now been collected and processed (https://docs.google.com/document/d/1D5SiNbeDEfY2hTWquS88MGUrLv6IgZzfCzflppRGb5k/edit). In order to use these features, knowledge has to be obtained for the requirements on said features including, but not limited to merging to the trianing dataset and storage.\n",
    "\n",
    "- Acceptance criterea:\n",
    "    - Does not take up terrabytes of storage.\n",
    "        - _If it does, data sources might have to be merged 1 by 1, removing the source after the merge._\n",
    "    - Can be merged without 3rd party software (tabix, python etc. allowed).\n",
    "    - Data can easily be downloaded.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "## As student / Robert I want to find commonalities between bad performing genes in CAPICE\n",
    "\n",
    "- Content:\n",
    "\n",
    "The enrichr tool has proven to be quite beneficial to understanding a set of bad performing genes. Further processing of these enrichr results is required in order to fully understand the underlying mechanics and commonalities that these bad performing genes share.\n",
    "\n",
    "- Acceptance criterea:\n",
    "    - Enrichr is performed on multiple gene sets of different AUC thresholds (0.5, 0.6, 0.7 etc.).\n",
    "    - Results of various data sources (kegg, bioplanet, wikipathways etc.) have been obtained in an automated way using the Enrichr API.\n",
    "    - Results from the Enrichr API have been processed in an understandable plot, an mockup can be found [here](https://docs.google.com/presentation/d/1_woKoD9nqWnxueyFYfwyX6B4owg8vYF7rw_nKpMPux4/edit#slide=id.p).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichr automated API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichr_url = 'http://amp.pharm.mssm.edu/Enrichr/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>default_auc</th>\n",
       "      <th>default_f1</th>\n",
       "      <th>default_recall</th>\n",
       "      <th>default_fpr</th>\n",
       "      <th>default_spec</th>\n",
       "      <th>optimal_c</th>\n",
       "      <th>optimal_auc</th>\n",
       "      <th>optimal_f1</th>\n",
       "      <th>optimal_recall</th>\n",
       "      <th>optimal_fpr</th>\n",
       "      <th>optimal_spec</th>\n",
       "      <th>n_tot</th>\n",
       "      <th>n_benign</th>\n",
       "      <th>n_malign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2775</th>\n",
       "      <td>PRKN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>BUB1B</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73</td>\n",
       "      <td>66</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>GALNT3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>PNPLA1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>TMEM80</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>C10orf2</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>SPTLC2</td>\n",
       "      <td>0.455157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.750374</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>PLCD1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>UBE2A</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>SOX9-AS1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2807 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          gene  default_auc  default_f1  default_recall  default_fpr  \\\n",
       "2775      PRKN     1.000000         1.0             1.0          0.0   \n",
       "1659     BUB1B     1.000000         1.0             1.0          0.0   \n",
       "2102    GALNT3     1.000000         1.0             1.0          0.0   \n",
       "1655    PNPLA1     1.000000         1.0             1.0          0.0   \n",
       "2782    TMEM80     1.000000         1.0             1.0          0.0   \n",
       "...        ...          ...         ...             ...          ...   \n",
       "2625   C10orf2     0.469697         0.0             0.0          1.0   \n",
       "2330    SPTLC2     0.455157         0.0             0.0          1.0   \n",
       "2615     PLCD1     0.500000         0.0             0.0          1.0   \n",
       "2611     UBE2A     0.500000         0.0             0.0          1.0   \n",
       "2806  SOX9-AS1     0.000000         0.0             0.0          1.0   \n",
       "\n",
       "      default_spec  optimal_c  optimal_auc  optimal_f1  optimal_recall  \\\n",
       "2775           1.0      0.020     1.000000    1.000000             1.0   \n",
       "1659           1.0      0.020     1.000000    1.000000             1.0   \n",
       "2102           1.0      0.020     1.000000    1.000000             1.0   \n",
       "1655           1.0      0.020     1.000000    1.000000             0.8   \n",
       "2782           1.0      0.020     1.000000    1.000000             1.0   \n",
       "...            ...        ...          ...         ...             ...   \n",
       "2625           0.0      0.020     0.469697    0.000000             0.0   \n",
       "2330           0.0      0.011     0.750374    0.170213             0.0   \n",
       "2615           0.0      0.020     0.500000    0.000000             0.0   \n",
       "2611           0.0      0.020     0.500000    0.000000             0.0   \n",
       "2806           0.0      0.020     0.000000    0.000000             0.0   \n",
       "\n",
       "      optimal_fpr  optimal_spec  n_tot  n_benign  n_malign  \n",
       "2775          0.0           1.0     11         7         4  \n",
       "1659          0.0           1.0     73        66         7  \n",
       "2102          0.0           1.0     12        10         2  \n",
       "1655          0.2           0.8     31        26         5  \n",
       "2782          0.0           1.0      5         4         1  \n",
       "...           ...           ...    ...       ...       ...  \n",
       "2625          1.0           0.0    133       132         1  \n",
       "2330          1.0           0.0    229       223         6  \n",
       "2615          1.0           0.0      8         7         1  \n",
       "2611          1.0           0.0      3         2         1  \n",
       "2806          1.0           0.0      3         2         1  \n",
       "\n",
       "[2807 rows x 15 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in initial data\n",
    "data = pd.read_csv(read_loc+'optimal_f1_full_ds_v2.csv', header=0)\n",
    "data.sort_values(by='default_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2807"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of lists of genes of interest with as key the 0.x threshold\n",
    "\n",
    "auc_dict = {}\n",
    "thresholds = np.arange(0.5, 1.01, 0.05).round(decimals=2)\n",
    "for t in thresholds:\n",
    "    list_of_genes = data[data['default_auc'] < t]['gene'].tolist()\n",
    "    auc_dict[t.astype('str')] = '\\n'.join(list_of_genes)\n",
    "    del list_of_genes\n",
    "del thresholds\n",
    "\n",
    "# Save it to a file so I don't bombard the Enrichr API with requests\n",
    "if not os.path.isfile('./user_id_list.json'):\n",
    "    file = open('./user_id_list.json', 'w')\n",
    "    file.close()\n",
    "\n",
    "with open('./user_id_list.json') as json_file:\n",
    "    try:\n",
    "        user_list_ids = json.load(json_file)\n",
    "        json_was_empty = False\n",
    "    except json.JSONDecodeError:\n",
    "        user_list_ids = {}\n",
    "        json_was_empty = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list ID's for the different AUC's\n",
    "\n",
    "for threshold in auc_dict.keys():\n",
    "    if threshold not in user_list_ids.keys():\n",
    "        description = f'CAPICE performance of genes with an AUC lower than {threshold}'\n",
    "        payload = {\n",
    "            'list': (None, auc_dict[threshold]),\n",
    "            'description': (None, description)\n",
    "        }\n",
    "        response = requests.post(enrichr_url + '/addList', files=payload)\n",
    "        if not response.ok:\n",
    "            print(f'Something went wrong with {threshold}!')\n",
    "        else:\n",
    "            enrichr_addlist_response = json.loads(response.text)\n",
    "            user_list_ids[threshold] = enrichr_addlist_response['userListId']\n",
    "\n",
    "# Write it to a file in case I didn't have any.\n",
    "if json_was_empty:\n",
    "    with open('./user_id_list.json', 'w') as json_file:\n",
    "        json.dump(user_list_ids, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to get the enrichment results.\n",
    "\n",
    "enrichr_sources_oi = ['BioPlanet_2019', 'WikiPathways_2019_Human', 'KEGG_2019_Human', 'Elsevier_Pathway_Collection',\n",
    "                     'GO_Biological_Process_2018',\n",
    "                     'PheWeb_2019', 'ClinVar_2019', 'GWAS_Catalog_2019', 'DisGeNET',\n",
    "                     'Human_Gene_Atlas', 'ARCHS4_Tissues']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the enrichr results.\n",
    "def get_enrichr_results(enrichr_sources, number_of_hits):\n",
    "    if len(enrichr_sources) == 0:\n",
    "        print('You should give some sources for me to get data from.')\n",
    "    else:\n",
    "\n",
    "        # Prepare output dataframes for each source\n",
    "        output_dir = '/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/'\n",
    "        \n",
    "        for iteration, source in enumerate(enrichr_sources):\n",
    "            print(f'Working on source: {source}')\n",
    "            if iteration == len(enrichr_sources):\n",
    "                sources_left = []\n",
    "            else:\n",
    "                sources_left = enrichr_sources[iteration+1::]\n",
    "            print(f'Sources left: {sources_left}')\n",
    "            # Prepare the output dict\n",
    "            current_source_df = pd.DataFrame(columns=['AUC',\n",
    "                                                      'Rank',\n",
    "                                                      'Term_Name',\n",
    "                                                      'Overlapping_Genes',\n",
    "                                                      'Adjusted_P-value',\n",
    "                                                      'Source'])\n",
    "            for (threshold, user_id) in zip(user_list_ids.keys(),user_list_ids.values()):\n",
    "                print(f'\\tCurrently working on threshold: {threshold}')\n",
    "                req = enrichr_url + f'enrich?userListId={user_id}&backgroundType={source}'\n",
    "                response = requests.get(req)\n",
    "                if not response.ok:\n",
    "                    print(f'Userid {user_id} (threshold: {threshold})'\n",
    "                          f'received error on: {source}!')\n",
    "                else:\n",
    "                    enrichr_response_data = json.loads(response.text)\n",
    "                    content = enrichr_response_data[source]\n",
    "                    auc = threshold\n",
    "                    content_df = pd.DataFrame(columns=['AUC', 'Rank', 'Term_Name',\n",
    "                                                       'Combined_score',\n",
    "                                                       'Overlapping_Genes',\n",
    "                                                       'Adjusted_P-value',\n",
    "                                                       'Source'])\n",
    "                    for c in content:\n",
    "                        rank = c[0]\n",
    "                        term_name = c[1]\n",
    "                        combined_score = c[4]\n",
    "                        overlapping_genes = ', '.join(c[5])\n",
    "                        a_p_value = c[6]\n",
    "                        local_df = pd.DataFrame({'AUC':auc, 'Rank':rank,\n",
    "                                                 'Term_Name': term_name,\n",
    "                                                 'Combined_score': combined_score,\n",
    "                                                 'Overlapping_Genes': overlapping_genes,\n",
    "                                                 'Adjusted_P-value': a_p_value,\n",
    "                                                'Source': source}, index=[0])\n",
    "                        content_df = content_df.append(local_df, ignore_index=True)\n",
    "                    content_df = content_df.sort_values(by=['Adjusted_P-value',\n",
    "                                                            'Combined_score'],\n",
    "                                                        ascending=[True, False])\n",
    "                    max_rows = 0\n",
    "                    if content_df.shape[0] < number_of_hits:\n",
    "                        max_rows = content_df.shape[0] - 1\n",
    "                    else:\n",
    "                        max_rows = number_of_hits\n",
    "                    out_df = content_df[:max_rows]\n",
    "                    out_df.drop('Combined_score', axis=1, inplace=True)\n",
    "                    current_source_df = current_source_df.append(out_df, ignore_index=True)\n",
    "            output_filename = output_dir + source + '.csv'\n",
    "            current_source_df.to_csv(output_filename, index=False, sep=',')\n",
    "            print(f'\\tSource {source} has been exported to csv (\\n\\t\\t{output_filename}\\n\\t)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_source_link = {}\n",
    "for source in enrichr_sources_oi:\n",
    "    output_filename = source + '.csv'\n",
    "    output_source_link[source] = output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_files_present = glob.glob('./EnrichrAPIResults/*.csv')\n",
    "list_of_files_present = []\n",
    "if len(glob_files_present) > 0:\n",
    "    for file in glob_files_present:\n",
    "        list_of_files_present.append(file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_required = []\n",
    "for source, output_filename in output_source_link.items():\n",
    "    if output_filename not in list_of_files_present:\n",
    "        sources_required.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BioPlanet_2019',\n",
       " 'WikiPathways_2019_Human',\n",
       " 'KEGG_2019_Human',\n",
       " 'Elsevier_Pathway_Collection',\n",
       " 'GO_Biological_Process_2018',\n",
       " 'PheWeb_2019',\n",
       " 'ClinVar_2019',\n",
       " 'GWAS_Catalog_2019',\n",
       " 'DisGeNET',\n",
       " 'Human_Gene_Atlas',\n",
       " 'ARCHS4_Tissues']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources_required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on source: BioPlanet_2019\n",
      "Sources left: ['WikiPathways_2019_Human', 'KEGG_2019_Human', 'Elsevier_Pathway_Collection', 'GO_Biological_Process_2018', 'PheWeb_2019', 'ClinVar_2019', 'GWAS_Catalog_2019', 'DisGeNET', 'Human_Gene_Atlas', 'ARCHS4_Tissues']\n",
      "\tCurrently working on threshold: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjsietsma/PycharmProjects/dsls_master_thesis/venv/lib/python3.8/site-packages/pandas/core/frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource BioPlanet_2019 has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/BioPlanet_2019.csv\n",
      "\t)\n",
      "Working on source: WikiPathways_2019_Human\n",
      "Sources left: ['KEGG_2019_Human', 'Elsevier_Pathway_Collection', 'GO_Biological_Process_2018', 'PheWeb_2019', 'ClinVar_2019', 'GWAS_Catalog_2019', 'DisGeNET', 'Human_Gene_Atlas', 'ARCHS4_Tissues']\n",
      "\tCurrently working on threshold: 0.5\n",
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource WikiPathways_2019_Human has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/WikiPathways_2019_Human.csv\n",
      "\t)\n",
      "Working on source: KEGG_2019_Human\n",
      "Sources left: ['Elsevier_Pathway_Collection', 'GO_Biological_Process_2018', 'PheWeb_2019', 'ClinVar_2019', 'GWAS_Catalog_2019', 'DisGeNET', 'Human_Gene_Atlas', 'ARCHS4_Tissues']\n",
      "\tCurrently working on threshold: 0.5\n",
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource KEGG_2019_Human has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/KEGG_2019_Human.csv\n",
      "\t)\n",
      "Working on source: Elsevier_Pathway_Collection\n",
      "Sources left: ['GO_Biological_Process_2018', 'PheWeb_2019', 'ClinVar_2019', 'GWAS_Catalog_2019', 'DisGeNET', 'Human_Gene_Atlas', 'ARCHS4_Tissues']\n",
      "\tCurrently working on threshold: 0.5\n",
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource Elsevier_Pathway_Collection has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/Elsevier_Pathway_Collection.csv\n",
      "\t)\n",
      "Working on source: GO_Biological_Process_2018\n",
      "Sources left: ['PheWeb_2019', 'ClinVar_2019', 'GWAS_Catalog_2019', 'DisGeNET', 'Human_Gene_Atlas', 'ARCHS4_Tissues']\n",
      "\tCurrently working on threshold: 0.5\n",
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource GO_Biological_Process_2018 has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/GO_Biological_Process_2018.csv\n",
      "\t)\n",
      "Working on source: PheWeb_2019\n",
      "Sources left: ['ClinVar_2019', 'GWAS_Catalog_2019', 'DisGeNET', 'Human_Gene_Atlas', 'ARCHS4_Tissues']\n",
      "\tCurrently working on threshold: 0.5\n",
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource PheWeb_2019 has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/PheWeb_2019.csv\n",
      "\t)\n",
      "Working on source: ClinVar_2019\n",
      "Sources left: ['GWAS_Catalog_2019', 'DisGeNET', 'Human_Gene_Atlas', 'ARCHS4_Tissues']\n",
      "\tCurrently working on threshold: 0.5\n",
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource ClinVar_2019 has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/ClinVar_2019.csv\n",
      "\t)\n",
      "Working on source: GWAS_Catalog_2019\n",
      "Sources left: ['DisGeNET', 'Human_Gene_Atlas', 'ARCHS4_Tissues']\n",
      "\tCurrently working on threshold: 0.5\n",
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource GWAS_Catalog_2019 has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/GWAS_Catalog_2019.csv\n",
      "\t)\n",
      "Working on source: DisGeNET\n",
      "Sources left: ['Human_Gene_Atlas', 'ARCHS4_Tissues']\n",
      "\tCurrently working on threshold: 0.5\n",
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource DisGeNET has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/DisGeNET.csv\n",
      "\t)\n",
      "Working on source: Human_Gene_Atlas\n",
      "Sources left: ['ARCHS4_Tissues']\n",
      "\tCurrently working on threshold: 0.5\n",
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource Human_Gene_Atlas has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/Human_Gene_Atlas.csv\n",
      "\t)\n",
      "Working on source: ARCHS4_Tissues\n",
      "Sources left: []\n",
      "\tCurrently working on threshold: 0.5\n",
      "\tCurrently working on threshold: 0.55\n",
      "\tCurrently working on threshold: 0.6\n",
      "\tCurrently working on threshold: 0.65\n",
      "\tCurrently working on threshold: 0.7\n",
      "\tCurrently working on threshold: 0.75\n",
      "\tCurrently working on threshold: 0.8\n",
      "\tCurrently working on threshold: 0.85\n",
      "\tCurrently working on threshold: 0.9\n",
      "\tCurrently working on threshold: 0.95\n",
      "\tCurrently working on threshold: 1.0\n",
      "\tSource ARCHS4_Tissues has been exported to csv (\n",
      "\t\t/home/rjsietsma/PycharmProjects/dsls_master_thesis/side_scripts/EnrichrAPIResults/ARCHS4_Tissues.csv\n",
      "\t)\n"
     ]
    }
   ],
   "source": [
    "get_enrichr_results(sources_required, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichr_data_loc = './EnrichrAPIResults/'\n",
    "\n",
    "enrichr_dataset = {}\n",
    "\n",
    "for source in enrichr_sources_oi:\n",
    "    dataset = enrichr_data_loc + source + '.csv'\n",
    "    enrichr_dataset[source] = pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Term_Name</th>\n",
       "      <th>Overlapping_Genes</th>\n",
       "      <th>Adjusted_P-value</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>Synaptic proteins at the synaptic junction</td>\n",
       "      <td>NRXN3, SPTAN1</td>\n",
       "      <td>0.90841</td>\n",
       "      <td>BioPlanet_2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>Activation of TRKA receptors</td>\n",
       "      <td>NTRK2</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>BioPlanet_2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>12</td>\n",
       "      <td>FGFR3b ligand binding and activation</td>\n",
       "      <td>FGF9</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>BioPlanet_2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>13</td>\n",
       "      <td>Folate and pterine metabolism</td>\n",
       "      <td>SLC46A1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>BioPlanet_2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>14</td>\n",
       "      <td>Tat-mediated elongation of the HIV-1 transcript</td>\n",
       "      <td>CTDP1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>BioPlanet_2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC  Rank                                        Term_Name  \\\n",
       "0  0.5     1       Synaptic proteins at the synaptic junction   \n",
       "1  0.5    10                     Activation of TRKA receptors   \n",
       "2  0.5    12             FGFR3b ligand binding and activation   \n",
       "3  0.5    13                    Folate and pterine metabolism   \n",
       "4  0.5    14  Tat-mediated elongation of the HIV-1 transcript   \n",
       "\n",
       "  Overlapping_Genes  Adjusted_P-value          Source  \n",
       "0     NRXN3, SPTAN1           0.90841  BioPlanet_2019  \n",
       "1             NTRK2           1.00000  BioPlanet_2019  \n",
       "2              FGF9           1.00000  BioPlanet_2019  \n",
       "3           SLC46A1           1.00000  BioPlanet_2019  \n",
       "4             CTDP1           1.00000  BioPlanet_2019  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enrichr_dataset['BioPlanet_2019'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(source, n=5, print_messages=False):\n",
    "    source_name = source['Source'][0]\n",
    "    term_values = source['Term_Name'].value_counts()\n",
    "    if print_messages:\n",
    "        print(f'The most occuring term names in {source_name} is: \\n{term_values[:n]}')\n",
    "    plot_source = pd.DataFrame(columns=source.columns)\n",
    "    for auc in source['AUC'].unique():\n",
    "        subsource = source[source['AUC'] == auc][:n]\n",
    "        plot_source = plot_source.append(subsource)\n",
    "    category = 'Term_Name'\n",
    "    category_items = plot_source[category].unique()\n",
    "    palette = inferno(len(category_items) + 1)\n",
    "    colormap = dict(zip(category_items, palette))\n",
    "    plot_source['color'] = plot_source[category].map(colormap)\n",
    "    \n",
    "    title = f'Term names from {source_name}, n={n}'\n",
    "    source_bokeh = ColumnDataSource(plot_source)\n",
    "    hover = HoverTool(tooltips=[('Term_name', '@Term_Name'), ('Genes', '@Overlapping_Genes')])\n",
    "    tools = [hover, WheelZoomTool(), PanTool(), BoxZoomTool(), ResetTool(), SaveTool()]\n",
    "    \n",
    "    p = figure(tools=tools, title=title, plot_width=2000, plot_height=1200,toolbar_location='right', toolbar_sticky=False,)\n",
    "    if n > 5:\n",
    "        p.scatter(x='AUC',y='Adjusted_P-value', source=source_bokeh, size=10, color='color')\n",
    "    else:\n",
    "        p.scatter(x='AUC',y='Adjusted_P-value', source=source_bokeh, size=10, color='color', legend=category)\n",
    "    p.xaxis.axis_label = 'AUC'\n",
    "    p.yaxis.axis_label = 'Adjusted P-value'\n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_pictures_present = glob.glob('./EnrichrAPIResults/*.html')\n",
    "list_of_pictures_present = []\n",
    "if len(glob_pictures_present) > 0:\n",
    "    for file in glob_pictures_present:\n",
    "        list_of_pictures_present.append(file)\n",
    "output_picture_link = {}\n",
    "for source in enrichr_sources_oi:\n",
    "    output_filename = './EnrichrAPIResults/' + source + '.html'\n",
    "    output_picture_link[source] = output_filename\n",
    "pictures_required = []\n",
    "for source, output_picture in output_picture_link.items():\n",
    "    if output_picture not in list_of_pictures_present:\n",
    "        pictures_required.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BioPlanet_2019': './EnrichrAPIResults/BioPlanet_2019.html',\n",
       " 'WikiPathways_2019_Human': './EnrichrAPIResults/WikiPathways_2019_Human.html',\n",
       " 'KEGG_2019_Human': './EnrichrAPIResults/KEGG_2019_Human.html',\n",
       " 'Elsevier_Pathway_Collection': './EnrichrAPIResults/Elsevier_Pathway_Collection.html',\n",
       " 'GO_Biological_Process_2018': './EnrichrAPIResults/GO_Biological_Process_2018.html',\n",
       " 'PheWeb_2019': './EnrichrAPIResults/PheWeb_2019.html',\n",
       " 'ClinVar_2019': './EnrichrAPIResults/ClinVar_2019.html',\n",
       " 'GWAS_Catalog_2019': './EnrichrAPIResults/GWAS_Catalog_2019.html',\n",
       " 'DisGeNET': './EnrichrAPIResults/DisGeNET.html',\n",
       " 'Human_Gene_Atlas': './EnrichrAPIResults/Human_Gene_Atlas.html',\n",
       " 'ARCHS4_Tissues': './EnrichrAPIResults/ARCHS4_Tissues.html'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_picture_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "/home/rjsietsma/PycharmProjects/dsls_master_thesis/venv/lib/python3.8/site-packages/bokeh/io/saving.py:125: UserWarning: save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\n",
      "  warn(\"save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\")\n",
      "/home/rjsietsma/PycharmProjects/dsls_master_thesis/venv/lib/python3.8/site-packages/bokeh/io/saving.py:138: UserWarning: save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\n",
      "  warn(\"save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\")\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n"
     ]
    }
   ],
   "source": [
    "if len(pictures_required) > 0:\n",
    "    for plot_req in pictures_required:\n",
    "        plot = plot_results(enrichr_dataset[plot_req], 5)\n",
    "        save(plot, output_picture_link[plot_req])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "451.85px",
    "left": "1759px",
    "right": "20px",
    "top": "91px",
    "width": "780px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}